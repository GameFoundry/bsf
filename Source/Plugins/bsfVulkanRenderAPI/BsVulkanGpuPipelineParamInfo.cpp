//************************************ bs::framework - Copyright 2018 Marko Pintera **************************************//
//*********** Licensed under the MIT license. See LICENSE.md for full terms. This notice is not to be removed. ***********//
#include "BsVulkanGpuPipelineParamInfo.h"
#include "BsVulkanUtility.h"
#include "BsVulkanRenderAPI.h"
#include "BsVulkanDevice.h"
#include "RenderAPI/BsGpuParamDesc.h"

namespace bs { namespace ct
{
	VulkanGpuPipelineParamInfo::VulkanGpuPipelineParamInfo(const GPU_PIPELINE_PARAMS_DESC& desc, GpuDeviceFlags deviceMask)
		: GpuPipelineParamInfo(desc, deviceMask), mDeviceMask(deviceMask), mLayouts(), mLayoutInfos()
	{ }

	void VulkanGpuPipelineParamInfo::initialize()
	{
		VulkanRenderAPI& rapi = static_cast<VulkanRenderAPI&>(RenderAPI::instance());

		VulkanDevice* devices[BS_MAX_DEVICES];
		VulkanUtility::getDevices(rapi, mDeviceMask, devices);

		UINT32 numDevices = 0;
		for (UINT32 i = 0; i < BS_MAX_DEVICES; i++)
		{
			if (devices[i] != nullptr)
				numDevices++;
		}

		UINT32 totalNumSlots = 0;
		for (UINT32 i = 0; i < mNumSets; i++)
			totalNumSlots += mSetInfos[i].numSlots;

		mAlloc.reserve<VkDescriptorSetLayoutBinding>(mNumElements)
			.reserve<GpuParamObjectType>(mNumElements)
			.reserve<GpuBufferFormat>(mNumElements)
			.reserve<LayoutInfo>(mNumSets)
			.reserve<VulkanDescriptorLayout*>(mNumSets * numDevices)
			.reserve<SetExtraInfo>(mNumSets)
			.reserve<UINT32>(totalNumSlots)
			.init();

		mLayoutInfos = mAlloc.alloc<LayoutInfo>(mNumSets);
		VkDescriptorSetLayoutBinding* bindings = mAlloc.alloc<VkDescriptorSetLayoutBinding>(mNumElements);
		GpuParamObjectType* types = mAlloc.alloc<GpuParamObjectType>(mNumElements);
		GpuBufferFormat* elementTypes = mAlloc.alloc<GpuBufferFormat>(mNumElements);

		for (UINT32 i = 0; i < BS_MAX_DEVICES; i++)
		{
			if (devices[i] == nullptr)
			{
				mLayouts[i] = nullptr;
				continue;
			}

			mLayouts[i] = mAlloc.alloc<VulkanDescriptorLayout*>(mNumSets);
		}

		mSetExtraInfos = mAlloc.alloc<SetExtraInfo>(mNumSets);

		if(bindings != nullptr)
			bs_zero_out(bindings, mNumElements);

		if (types != nullptr)
			bs_zero_out(types, mNumElements);

		if (elementTypes != nullptr)
			bs_zero_out(elementTypes, mNumElements);

		UINT32 globalBindingIdx = 0;
		for (UINT32 i = 0; i < mNumSets; i++)
		{
			mSetExtraInfos[i].slotIndices = mAlloc.alloc<UINT32>(mSetInfos[i].numSlots);

			mLayoutInfos[i].numBindings = 0;
			mLayoutInfos[i].bindings = nullptr;
			mLayoutInfos[i].types = nullptr;
			mLayoutInfos[i].elementTypes = nullptr;

			for (UINT32 j = 0; j < mSetInfos[i].numSlots; j++)
			{
				if (mSetInfos[i].slotIndices[j] == (UINT32)-1)
				{
					mSetExtraInfos[i].slotIndices[j] = (UINT32)-1;
					continue;
				}

				VkDescriptorSetLayoutBinding& binding = bindings[globalBindingIdx];
				binding.binding = j;

				mSetExtraInfos[i].slotIndices[j] = globalBindingIdx;
				mLayoutInfos[i].numBindings++;
				globalBindingIdx++;
			}
		}

		UINT32 offset = 0;
		for (UINT32 i = 0; i < mNumSets; i++)
		{
			mLayoutInfos[i].bindings = &bindings[offset];
			mLayoutInfos[i].types = &types[offset];
			mLayoutInfos[i].elementTypes = &elementTypes[offset];
			offset += mLayoutInfos[i].numBindings;
		}

		VkShaderStageFlags stageFlagsLookup[6];
		stageFlagsLookup[GPT_VERTEX_PROGRAM] = VK_SHADER_STAGE_VERTEX_BIT;
		stageFlagsLookup[GPT_HULL_PROGRAM] = VK_SHADER_STAGE_TESSELLATION_CONTROL_BIT;
		stageFlagsLookup[GPT_DOMAIN_PROGRAM] = VK_SHADER_STAGE_TESSELLATION_EVALUATION_BIT;
		stageFlagsLookup[GPT_GEOMETRY_PROGRAM] = VK_SHADER_STAGE_GEOMETRY_BIT;
		stageFlagsLookup[GPT_FRAGMENT_PROGRAM] = VK_SHADER_STAGE_FRAGMENT_BIT;
		stageFlagsLookup[GPT_COMPUTE_PROGRAM] = VK_SHADER_STAGE_COMPUTE_BIT;

		UINT32 numParamDescs = sizeof(mParamDescs) / sizeof(mParamDescs[0]);
		for (UINT32 i = 0; i < numParamDescs; i++)
		{
			const SPtr<GpuParamDesc>& paramDesc = mParamDescs[i];
			if (paramDesc == nullptr)
				continue;

			auto setUpBlockBindings = [&](auto& params, VkDescriptorType descType)
			{
				for (auto& entry : params)
				{
					UINT32 bindingIdx = getBindingIdx(entry.second.set, entry.second.slot);
					assert(bindingIdx != (UINT32)-1);

					VkDescriptorSetLayoutBinding& binding = bindings[bindingIdx];
					binding.descriptorCount = 1;
					binding.stageFlags |= stageFlagsLookup[i];
					binding.descriptorType = descType;
				}
			};

			auto setUpBindings = [&](auto& params, VkDescriptorType descType)
			{
				for (auto& entry : params)
				{
					UINT32 bindingIdx = getBindingIdx(entry.second.set, entry.second.slot);
					assert(bindingIdx != (UINT32)-1);

					VkDescriptorSetLayoutBinding& binding = bindings[bindingIdx];
					binding.descriptorCount = 1;
					binding.stageFlags |= stageFlagsLookup[i];
					binding.descriptorType = descType;

					types[bindingIdx] = entry.second.type;
					elementTypes[bindingIdx] = entry.second.elementType;
				}
			};

			setUpBlockBindings(paramDesc->paramBlocks, VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER);
			setUpBindings(paramDesc->textures, VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE);
			setUpBindings(paramDesc->loadStoreTextures, VK_DESCRIPTOR_TYPE_STORAGE_IMAGE);

			// Set up sampler bindings
			for (auto& entry : paramDesc->samplers)
			{
				UINT32 bindingIdx = getBindingIdx(entry.second.set, entry.second.slot);
				assert(bindingIdx != (UINT32)-1);

				VkDescriptorSetLayoutBinding& binding = bindings[bindingIdx];

				// If we already assigned an image to this binding slot, then it's a combined image/sampler
				if(binding.descriptorType == VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE)
					binding.descriptorType = VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER;
				else
				{
					binding.descriptorCount = 1;
					binding.stageFlags |= stageFlagsLookup[i];
					binding.descriptorType = VK_DESCRIPTOR_TYPE_SAMPLER;

					types[bindingIdx] = entry.second.type;
					elementTypes[bindingIdx] = entry.second.elementType;
				}
			}

			// Set up buffer bindings
			for (auto& entry : paramDesc->buffers)
			{
				UINT32 bindingIdx = getBindingIdx(entry.second.set, entry.second.slot);
				assert(bindingIdx != (UINT32)-1);

				VkDescriptorSetLayoutBinding& binding = bindings[bindingIdx];
				binding.descriptorCount = 1;
				binding.stageFlags |= stageFlagsLookup[i];

				switch(entry.second.type)
				{
				default:
				case GPOT_BYTE_BUFFER:
					binding.descriptorType = VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER;
					break;
				case GPOT_RWBYTE_BUFFER:
					binding.descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER;
					break;
				case GPOT_STRUCTURED_BUFFER:
				case GPOT_RWSTRUCTURED_BUFFER:
					binding.descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
					break;
				}

				types[bindingIdx] = entry.second.type;
				elementTypes[bindingIdx] = entry.second.elementType;
			}
		}

		// Allocate layouts per-device
		for (UINT32 i = 0; i < BS_MAX_DEVICES; i++)
		{
			if (mLayouts[i] == nullptr)
				continue;

			VulkanDescriptorManager& descManager = devices[i]->getDescriptorManager();
			for (UINT32 j = 0; j < mNumSets; j++)
				mLayouts[i][j] = descManager.getLayout(mLayoutInfos[j].bindings, mLayoutInfos[j].numBindings);
		}
	}

	VulkanDescriptorLayout* VulkanGpuPipelineParamInfo::getLayout(UINT32 deviceIdx, UINT32 layoutIdx) const
	{
		if (deviceIdx >= BS_MAX_DEVICES || mLayouts[deviceIdx] == nullptr)
			return nullptr;

		return mLayouts[deviceIdx][layoutIdx];
	}
}}
